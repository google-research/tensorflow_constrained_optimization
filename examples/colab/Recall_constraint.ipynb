{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Lj5Lwtb1LKlm"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Constrained Optimization Authors. All Rights Reserved.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "\u003e http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7_R0jiDvLPwX"
      },
      "source": [
        "## Problem Setup\n",
        "\n",
        "This is a simple example of recall-constrained optimization on simulated data: we seek a classifier that minimizes the average hinge loss while constraining recall to be at least 90%.\n",
        "\n",
        "We'll start with the required imports\u0026mdash;notice the definition of `tfco`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "fQi5HqA7LRhN"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from six.moves import xrange\n",
        "import tensorflow as tf\n",
        "\n",
        "# Use the GitHub version of TFCO\n",
        "!pip install git+https://github.com/google-research/tensorflow_constrained_optimization\n",
        "import tensorflow_constrained_optimization as tfco"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "npiEaDa8LT3-"
      },
      "source": [
        "We'll next create a simple simulated dataset by sampling 1000 random 10-dimensional feature vectors from a Gaussian, finding their labels using a random \"ground truth\" linear model, and then adding noise by randomly flipping 200 labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "0ZiNAGelLVgB"
      },
      "outputs": [],
      "source": [
        "# Create a simulated 10-dimensional training dataset consisting of 1000 labeled\n",
        "# examples, of which 800 are labeled correctly and 200 are mislabeled.\n",
        "num_examples = 1000\n",
        "num_mislabeled_examples = 200\n",
        "dimension = 10\n",
        "# We will constrain the recall to be at least 90%.\n",
        "recall_lower_bound = 0.9\n",
        "\n",
        "# Create random \"ground truth\" parameters for a linear model.\n",
        "ground_truth_weights = np.random.normal(size=dimension) / math.sqrt(dimension)\n",
        "ground_truth_threshold = 0\n",
        "\n",
        "# Generate a random set of features for each example.\n",
        "features = np.random.normal(size=(num_examples, dimension)).astype(\n",
        "    np.float32) / math.sqrt(dimension)\n",
        "# Compute the labels from these features given the ground truth linear model.\n",
        "labels = (np.matmul(features, ground_truth_weights) \u003e\n",
        "          ground_truth_threshold).astype(np.float32)\n",
        "# Add noise by randomly flipping num_mislabeled_examples labels.\n",
        "mislabeled_indices = np.random.choice(\n",
        "    num_examples, num_mislabeled_examples, replace=False)\n",
        "labels[mislabeled_indices] = 1 - labels[mislabeled_indices]\n",
        "\n",
        "# Constant Tensors containing the labels and features.\n",
        "constant_labels = tf.constant(labels, dtype=tf.float32)\n",
        "constant_features = tf.constant(features, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mYWPrPtOLZez"
      },
      "source": [
        "We're now ready to construct our model, and the corresponding optimization problem. We'll use a linear model of the form $f(x) = w^T x - t$, where $w$ is the `weights`, and $t$ is the `threshold`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "VBLouwVYLbqI"
      },
      "outputs": [],
      "source": [
        "# Create variables containing the model parameters.\n",
        "weights = tf.Variable(tf.zeros(dimension), dtype=tf.float32, name=\"weights\")\n",
        "threshold = tf.Variable(0.0, dtype=tf.float32, name=\"threshold\")\n",
        "\n",
        "# The predictions are a nullary function returning a Tensor to support eager\n",
        "# mode. In graph mode, you can simply use:\n",
        "#   predictions = (\n",
        "#       tf.tensordot(constant_features, weights, axes=(1, 0)) - threshold)\n",
        "def predictions():\n",
        "  return tf.tensordot(constant_features, weights, axes=(1, 0)) - threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T9CJ1KwwLeSL"
      },
      "source": [
        "Now that we have the output of our linear model (in the `predictions` variable), we can move on to constructing the optimization problem. At this point, there are two ways to proceed:\n",
        "\n",
        "1.  We can use the rate helpers provided by the TFCO library. This is the easiest way to construct optimization problems based on *rates* (where a \"rate\" is the proportion of training examples on which some event occurs).\n",
        "2.  We could instead create an implementation of the [`ConstrainedMinimizationProblem`](https://github.com/google-research/tensorflow_constrained_optimization/tree/master/tensorflow_constrained_optimization/python/constrained_minimization_problem.py) interface. This is the most flexible approach. In particular, it is not limited to problems expressed in terms of rates.\n",
        "\n",
        "We'll now consider each of these two options in turn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gOD8kz4pLg_I"
      },
      "source": [
        "### Option 1: rate helpers\n",
        "\n",
        "The main motivation of TFCO is to make it easy to create and optimize constrained problems written in terms of linear combinations of *rates*, where a \"rate\" is the proportion of training examples on which an event occurs (e.g. the false positive rate, which is the number of negatively-labeled examples on which the model makes a positive prediction, divided by the number of negatively-labeled examples). Our current example (minimizing a hinge relaxation of the error rate subject to a recall constraint) is such a problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "e9VLzahZLgli"
      },
      "outputs": [],
      "source": [
        "# Like the predictions, in eager mode, the labels should be a nullary function\n",
        "# returning a Tensor. In graph mode, you can drop the lambda.\n",
        "context = tfco.rate_context(predictions, labels=lambda: constant_labels)\n",
        "problem = tfco.RateMinimizationProblem(\n",
        "    tfco.error_rate(context), [tfco.recall(context) \u003e= recall_lower_bound])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LWN1H_BfLoo5"
      },
      "source": [
        "The first argument of all rate-construction helpers ([`error_rate`](https://github.com/google-research/tensorflow_constrained_optimization/tree/master/tensorflow_constrained_optimization/python/rates/binary_rates.py) and [`recall`](https://github.com/google-research/tensorflow_constrained_optimization/tree/master/tensorflow_constrained_optimization/python/rates/binary_rates.py) are the ones used here) is a \"context\" object, which represents what we're taking the rate *of*. For example, in a fairness problem, we might wish to constrain the [`positive_prediction_rate`](https://github.com/google-research/tensorflow_constrained_optimization/tree/master/tensorflow_constrained_optimization/python/rates/binary_rates.py)s of two protected classes (i.e. two subsets of the data) to be similar. In that case, we would create a context representing the entire dataset, then call the context's `subset` method to create contexts for the two protected classes, and finally call the [`positive_prediction_rate`](https://github.com/google-research/tensorflow_constrained_optimization/tree/master/tensorflow_constrained_optimization/python/rates/binary_rates.py) helper on the two resulting contexts. We're not trying to do anything so advanced here, however, so we only create a single context, representing the entire dataset.\n",
        "\n",
        "In addition to the context, rate-construction helpers also take two optional named parameters\u0026mdash;not used here\u0026mdash;named `penalty_loss` and `constraint_loss`, of which the former is used to define the proxy constraints, and the latter the \"true\" constraints. These default to the hinge and zero-one losses, respectively. The consequence of this is that we will attempt to minimize the average hinge loss (a relaxation of the error rate using the `penalty_loss`), while constraining the *true* recall (using the `constraint_loss`) by essentially learning how much we should penalize the hinge-constrained recall (`penalty_loss`, again).\n",
        "\n",
        "The [`RateMinimizationProblem`](https://github.com/google-research/tensorflow_constrained_optimization/tree/master/tensorflow_constrained_optimization/python/rates/rate_minimization_problem.py) class implements the [`ConstrainedMinimizationProblem`](https://github.com/google-research/tensorflow_constrained_optimization/tree/master/tensorflow_constrained_optimization/python/constrained_minimization_problem.py) interface, and is constructed from a rate expression to be minimized (the first parameter), subject to a list of rate constraints (the second). Using this class is typically more convenient and readable than constructing a [`ConstrainedMinimizationProblem`](https://github.com/google-research/tensorflow_constrained_optimization/tree/master/tensorflow_constrained_optimization/python/constrained_minimization_problem.py) manually: the objects returned by `error_rate` and `recall`\u0026mdash;and all other rate-constructing and rate-combining functions\u0026mdash;can be manipulated using python arithmetic operators (e.g. \"`0.5 * tfco.error_rate(context1) - tfco.true_positive_rate(context2)`\"), or converted into a constraint using a comparison operator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BVmhfRSBLrN_"
      },
      "source": [
        "### Option 2: explicit [`ConstrainedMinimizationProblem`](https://github.com/google-research/tensorflow_constrained_optimization/tree/master/tensorflow_constrained_optimization/python/constrained_minimization_problem.py)\n",
        "\n",
        "For problems that cannot be easily expressed using the rate helpers, one could instead an explicit implementation of the [`ConstrainedMinimizationProblem`](https://github.com/google-research/tensorflow_constrained_optimization/tree/master/tensorflow_constrained_optimization/python/constrained_minimization_problem.py) interface. The current task (minimizing the average hinge loss subject to a recall constraint) is a rate-based problem, but for illustrative reasons we will show how to create a [`ConstrainedMinimizationProblem`](https://github.com/google-research/tensorflow_constrained_optimization/tree/master/tensorflow_constrained_optimization/python/constrained_minimization_problem.py) for this task.\n",
        "\n",
        "The constructor takes three parameters: a `Tensor` containing the classification labels (0 or 1) for every training example, another `Tensor` containing the model's predictions on every training example (sometimes called the \"logits\"), and the lower bound on recall that will be enforced using a constraint.\n",
        "\n",
        "As before, this implementation will contain both constraints *and* proxy constraints: the former represents the constraint that the true recall (defined in terms of the *number* of true positives) be at least `recall_lower_bound`, while the latter represents the same constraint, but on a hinge approximation of the recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7YHtmMkYLqqz"
      },
      "outputs": [],
      "source": [
        "class ExampleProblem(tfco.ConstrainedMinimizationProblem):\n",
        "\n",
        "  def __init__(self, labels, predictions, recall_lower_bound):\n",
        "    self._labels = labels\n",
        "    self._predictions = predictions\n",
        "    self._recall_lower_bound = recall_lower_bound\n",
        "    # The number of positively-labeled examples.\n",
        "    self._positive_count = tf.reduce_sum(self._labels)\n",
        "\n",
        "  @property\n",
        "  def num_constraints(self):\n",
        "    return 1\n",
        "\n",
        "  def objective(self):\n",
        "    # In eager mode, the predictions must be a nullary function returning a\n",
        "    # Tensor. In graph mode, they could be either such a function, or a Tensor\n",
        "    # itself.\n",
        "    predictions = self._predictions\n",
        "    if callable(predictions):\n",
        "      predictions = predictions()\n",
        "    return tf.compat.v1.losses.hinge_loss(labels=self._labels,\n",
        "                                          logits=predictions)\n",
        "\n",
        "  def constraints(self):\n",
        "    # In eager mode, the predictions must be a nullary function returning a\n",
        "    # Tensor. In graph mode, they could be either such a function, or a Tensor\n",
        "    # itself.\n",
        "    predictions = self._predictions\n",
        "    if callable(predictions):\n",
        "      predictions = predictions()\n",
        "    # Recall that the labels are binary (0 or 1).\n",
        "    true_positives = self._labels * tf.cast(predictions \u003e 0, dtype=tf.float32)\n",
        "    true_positive_count = tf.reduce_sum(true_positives)\n",
        "    recall = true_positive_count / self._positive_count\n",
        "    # The constraint is (recall \u003e= self._recall_lower_bound), which we convert\n",
        "    # to (self._recall_lower_bound - recall \u003c= 0) because\n",
        "    # ConstrainedMinimizationProblems must always provide their constraints in\n",
        "    # the form (tensor \u003c= 0).\n",
        "    #\n",
        "    # The result of this function should be a tensor, with each element being\n",
        "    # a quantity that is constrained to be non-positive. We only have one\n",
        "    # constraint, so we return a one-element tensor.\n",
        "    return self._recall_lower_bound - recall\n",
        "\n",
        "  def proxy_constraints(self):\n",
        "    # In eager mode, the predictions must be a nullary function returning a\n",
        "    # Tensor. In graph mode, they could be either such a function, or a Tensor\n",
        "    # itself.\n",
        "    predictions = self._predictions\n",
        "    if callable(predictions):\n",
        "      predictions = predictions()\n",
        "    # Use 1 - hinge since we're SUBTRACTING recall in the constraint function,\n",
        "    # and we want the proxy constraint function to be convex. Recall that the\n",
        "    # labels are binary (0 or 1).\n",
        "    true_positives = self._labels * tf.minimum(1.0, predictions)\n",
        "    true_positive_count = tf.reduce_sum(true_positives)\n",
        "    recall = true_positive_count / self._positive_count\n",
        "    # Please see the corresponding comment in the constraints property.\n",
        "    return self._recall_lower_bound - recall\n",
        "\n",
        "problem = ExampleProblem(\n",
        "    labels=constant_labels,\n",
        "    predictions=predictions,\n",
        "    recall_lower_bound=recall_lower_bound,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CQ2usdwLlrLw"
      },
      "source": [
        "## Wrapping up\n",
        "\n",
        "We're almost ready to construct and train our model, but first we'll create a couple of functions to measure its performance. We're interested in two quantities: the average hinge loss (which we seek to minimize), and the recall (which we constrain)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Q-rBjHFnLvT1"
      },
      "outputs": [],
      "source": [
        "def average_hinge_loss(labels, predictions):\n",
        "  # Recall that the labels are binary (0 or 1).\n",
        "  signed_labels = (labels * 2) - 1\n",
        "  return np.mean(np.maximum(0.0, 1.0 - signed_labels * predictions))\n",
        "\n",
        "def recall(labels, predictions):\n",
        "  # Recall that the labels are binary (0 or 1).\n",
        "  positive_count = np.sum(labels)\n",
        "  true_positives = labels * (predictions \u003e 0)\n",
        "  true_positive_count = np.sum(true_positives)\n",
        "  return true_positive_count / positive_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j2zlOQ5CLySp"
      },
      "source": [
        "As was mentioned in [README.md](https://github.com/google-research/tensorflow_constrained_optimization/tree/master/README.md), a Lagrangian optimizer ([lagrangian_optimizer.py](https://github.com/google-research/tensorflow_constrained_optimization/tree/master/tensorflow_constrained_optimization/python/train/lagrangian_optimizer.py)) often suffices for problems without proxy constraints, but a proxy-Lagrangian optimizer ([proxy_lagrangian_optimizer.py](https://github.com/google-research/tensorflow_constrained_optimization/tree/master/tensorflow_constrained_optimization/python/train/proxy_lagrangian_optimizer.py)) is recommended for problems *with* proxy constraints. Since this problem contains proxy constraints, we use a proxy-Lagrangian optimizer.\n",
        "\n",
        "For this problem, the constraint is fairly easy to satisfy, so we can use the same \"inner\" optimizer (an `AdagradOptimizer` with a learning rate of 1) for optimization of both the model parameters (`weights` and `threshold`), and the internal parameters associated with the constraints (these are the analogues of the Lagrange multipliers used by the proxy-Lagrangian optimizer). For more difficult problems, it will often be necessary to use different optimizers, with different learning rates (presumably found via a hyperparameter search): to accomplish this, pass *both* the `optimizer` and `constraint_optimizer` parameters to `ProxyLagrangianOptimizerV1`'s (or `ProxyLagrangianOptimizerV2`'s) constructor.\n",
        "\n",
        "Since this is a convex problem (both the objective and proxy constraint functions are convex), we can just take the last iterate. Periodic snapshotting, and the use of the [`find_best_candidate_distribution`](https://github.com/google-research/tensorflow_constrained_optimization/tree/master/tensorflow_constrained_optimization/python/candidates.py) or [`find_best_candidate_index`](https://github.com/google-research/tensorflow_constrained_optimization/tree/master/tensorflow_constrained_optimization/python/candidates.py) functions, is generally only necessary for non-convex problems (and even then, it isn't *always* necessary)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 38920,
          "status": "ok",
          "timestamp": 1597788533810,
          "user": {
            "displayName": "Andrew Cotter",
            "photoUrl": "",
            "userId": "01600804956648002768"
          },
          "user_tz": 420
        },
        "id": "5BWi-QAbL0rh",
        "outputId": "4ae599b7-1b98-4d9c-e4d1-d8a2a124ee23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Constrained average hinge loss = 0.722943\n",
            "Constrained recall = 0.900826\n"
          ]
        }
      ],
      "source": [
        "if tf.executing_eagerly():\n",
        "  # In eager mode, we use a V2 optimizer (a tf.keras.optimizers.Optimizer). A V1\n",
        "  # optimizer, however, would work equally well.\n",
        "  optimizer = tfco.ProxyLagrangianOptimizerV2(\n",
        "      optimizer=tf.keras.optimizers.Adagrad(learning_rate=1.0),\n",
        "      num_constraints=problem.num_constraints)\n",
        "  # In addition to the model parameters (weights and threshold), we also need to\n",
        "  # optimize over any trainable variables associated with the problem (e.g.\n",
        "  # implicit slack variables and weight denominators), and those associated with\n",
        "  # the optimizer (the analogues of the Lagrange multipliers used by the\n",
        "  # proxy-Lagrangian formulation).\n",
        "  var_list = ([weights, threshold] + problem.trainable_variables +\n",
        "              optimizer.trainable_variables())\n",
        "\n",
        "  for ii in xrange(1000):\n",
        "    optimizer.minimize(problem, var_list=var_list)\n",
        "\n",
        "  trained_weights = weights.numpy()\n",
        "  trained_threshold = threshold.numpy()\n",
        "\n",
        "else:  # We're in graph mode.\n",
        "  # In graph mode, we use a V1 optimizer (a tf.compat.v1.train.Optimizer). A V2\n",
        "  # optimizer, however, would work equally well.\n",
        "  optimizer = tfco.ProxyLagrangianOptimizerV1(\n",
        "      optimizer=tf.compat.v1.train.AdagradOptimizer(learning_rate=1.0))\n",
        "  train_op = optimizer.minimize(problem)\n",
        "\n",
        "  with tf.Session() as session:\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    for ii in xrange(1000):\n",
        "      session.run(train_op)\n",
        "\n",
        "    trained_weights, trained_threshold = session.run((weights, threshold))\n",
        "\n",
        "trained_predictions = np.matmul(features, trained_weights) - trained_threshold\n",
        "print(\"Constrained average hinge loss = %f\" % average_hinge_loss(\n",
        "    labels, trained_predictions))\n",
        "print(\"Constrained recall = %f\" % recall(labels, trained_predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8h8gIJwiL4Dn"
      },
      "source": [
        "As we hoped, the recall is extremely close to 90%\u0026mdash;and, thanks to the fact that the optimizer uses a (hinge) proxy constraint only when needed, and the actual (zero-one) constraint whenever possible, this is the *true* recall, not a hinge approximation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1AJhsOZflWMM"
      },
      "source": [
        "### Unconstrained Model\n",
        "\n",
        "For comparison, let's try optimizing the same problem *without* the recall constraint:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 42428,
          "status": "ok",
          "timestamp": 1597788537331,
          "user": {
            "displayName": "Andrew Cotter",
            "photoUrl": "",
            "userId": "01600804956648002768"
          },
          "user_tz": 420
        },
        "id": "PaovuQyvMq1i",
        "outputId": "1440414d-f1d8-4fa7-ad42-a5c0feea4b44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unconstrained average hinge loss = 0.615532\n",
            "Unconstrained recall = 0.762397\n"
          ]
        }
      ],
      "source": [
        "if tf.executing_eagerly():\n",
        "  optimizer = tf.keras.optimizers.Adagrad(learning_rate=1.0)\n",
        "  var_list = [weights, threshold]\n",
        "\n",
        "  for ii in xrange(1000):\n",
        "    # For optimizing the unconstrained problem, we just minimize the\n",
        "    # \"objective\" portion of the minimization problem.\n",
        "    optimizer.minimize(problem.objective, var_list=var_list)\n",
        "\n",
        "  trained_weights = weights.numpy()\n",
        "  trained_threshold = threshold.numpy()\n",
        "\n",
        "else:  # We're in graph mode.\n",
        "  optimizer = tf.compat.v1.train.AdagradOptimizer(learning_rate=1.0)\n",
        "  # For optimizing the unconstrained problem, we just minimize the \"objective\"\n",
        "  # portion of the minimization problem.\n",
        "  train_op = optimizer.minimize(problem.objective())\n",
        "\n",
        "  with tf.Session() as session:\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    for ii in xrange(1000):\n",
        "      session.run(train_op)\n",
        "\n",
        "    trained_weights, trained_threshold = session.run((weights, threshold))\n",
        "\n",
        "trained_predictions = np.matmul(features, trained_weights) - trained_threshold\n",
        "print(\"Unconstrained average hinge loss = %f\" % average_hinge_loss(\n",
        "    labels, trained_predictions))\n",
        "print(\"Unconstrained recall = %f\" % recall(labels, trained_predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dAJI6E9cL7ou"
      },
      "source": [
        "Because there is no constraint, the unconstrained problem does a better job of minimizing the average hinge loss, but naturally doesn't approach 90% recall."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Recall_constraint.ipynb",
      "provenance": [
        {
          "file_id": "14Vn8QtC-la_7iKQe02rgq8ZLRR1eA2lg",
          "timestamp": 1597790763639
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 2",
      "name": "python2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
